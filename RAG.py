# -*- coding: utf-8 -*-
"""chatbot using Mistral

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/tasabehahmed/chatbot-using-mistral.6f18ae90-821f-4413-879d-69876eeb4699.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251219/auto/storage/goog4_request%26X-Goog-Date%3D20251219T191003Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5212036dd724a8bf24d698b3e32a70fa79d6b425e3a4f972983452f7bc6c682454ceefe72c37ff5cc7f2c4b5bbf817c7b0d96b323a7e84b976e630b9752be4af3cde3967cfe3221634db6662fee097eb48af34d6e11f75c65d5cdab30a3a2a4316a032c403e5b3d1e316a0ad9568e45001f354bf8ce06e771034216207ac5e21491a39328310f3930f0057482292c35f3652880121c4babac6fafde0f4bf2cc044852013785c522e18aee93da6bb052728cc516d221bccd637cdd0f6288c9135f0d623da6d0078544a550f7b71647e485a18a1d60603ab4945cda64ef88923c8dcae873b3a3b8efd56865abf897f5288b939833b1a2e2fb6291d50d31b31cea1
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
#import kagglehub
#kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

#tasabehahmed_merged_path = kagglehub.dataset_download('tasabehahmed/merged')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

"""import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))"""

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""from huggingface_hub import login

login()"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "mistralai/Mistral-Nemo-Instruct-2407"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")


def generate_text(prompt, max_length=100, num_return_sequences=1):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(
        **inputs,
        max_length=max_length,
        num_return_sequences=num_return_sequences,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.3,
    )
    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]

#!pip install sentence-transformers PyPDF2 faiss-cpu

from sentence_transformers import SentenceTransformer
from PyPDF2 import PdfReader
import faiss
import numpy as np

def extract_text_from_pdf(pdf_path):
    reader = PdfReader(pdf_path)
    full_text = ""
    for page in reader.pages:
        full_text += page.extract_text() + "\n"
    return full_text

def chunk_text(text, chunk_size=500, overlap=50):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk = " ".join(words[i:i + chunk_size])
        chunks.append(chunk)
    return chunks

def embed_chunks(chunks, model_name='sentence-transformers/all-MiniLM-L6-v2'):
    model = SentenceTransformer(model_name)
    embeddings = model.encode(chunks, convert_to_numpy=True)
    return model, embeddings

def create_faiss_index(embeddings):
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    return index

def search_index(query, model, index, chunks, k=5):
    query_embedding = model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_embedding, k)
    return [chunks[i] for i in indices[0]]

excel_path = "/kaggle/input/merged/merged_chatbot_dataset.xlsx"
df = pd.read_excel(excel_path)
all_text = "\n".join(df.astype(str).values.flatten())

chunks = chunk_text(all_text, chunk_size=50, overlap=5)
model_embeddings, embeddings = embed_chunks(chunks)
index = create_faiss_index(embeddings)

def generate_response(query):
    docs = search_index(query, model_embeddings, index, chunks, k=3)
    context = "\n\n".join(docs)

    prompt = f"""
You are a bilingual assistant (Arabic + English).
Use ONLY the following context to answer the user's question briefly and clearly.

Your answer MUST follow these rules:
- Answer in ONE short sentence only.
- Very clear and direct.
- No bullet points.
- No lists.
- No explanation.
- No repeating the context.
- Use the SAME language as the question.
- If the context lists multiple options, present them in a structured way like (العربي أو بي تك أو زانوسي).

Context:
{context}

Question: {query}
Answer:
"""

    generated = generate_text(prompt, max_length=700)


    if isinstance(generated, str):
        full_text = generated
    elif isinstance(generated, list):
        if len(generated) > 0 and isinstance(generated[0], int):

            full_text = tokenizer.decode(generated[0], skip_special_tokens=True)
        elif len(generated) > 0 and isinstance(generated[0], str):

            full_text = ' '.join(generated)
        else:
            full_text = str(generated)
    else:
        full_text = str(generated)


    if "Answer:" in full_text:
        answer = full_text.split("Answer:")[1].strip()
    else:
        answer = full_text.strip()


    if answer.startswith("You are"):

        lines = answer.split('\n')
        for i, line in enumerate(lines):
            if line.strip() == "Answer:":
                answer = '\n'.join(lines[i+1:]).strip()
                break

    return answer

print(generate_response("ايه افضل شركة اجهزة كهربائية؟"))

print(generate_response("اشتري رخام منين"))

print(generate_response("Where can I buy ceramics?"))

evaluation_data = [
    {
        "question": "ايه افضل شركة اجهزة كهربائية؟",
        "expected_answer": "العربي أو بي تك أو زانوسي"
    },
    {
        "question": "Where can I buy ceramics?",
        "expected_answer": "Cleopatra Ceramics, El-Gawhara, Royal Ceramics."
    }
]

def retrieval_accuracy(eval_data):
    correct = 0
    for item in eval_data:
        docs = search_index(
            item["question"],
            model_embeddings,
            index,
            chunks,
            k=3
        )
        context = " ".join(docs)
        if item["expected_answer"].split()[0] in context:
            correct += 1
    return correct / len(eval_data)

from difflib import SequenceMatcher

def answer_accuracy(eval_data, threshold=0.5):
    correct = 0
    for item in eval_data:
        answer = generate_response(item["question"])
        similarity = SequenceMatcher(
            None,
            answer.lower(),
            item["expected_answer"].lower()
        ).ratio()
        if similarity >= threshold:
            correct += 1
    return correct / len(eval_data)

def faithfulness_score(eval_data):
    faithful = 0
    for item in eval_data:
        docs = search_index(
            item["question"],
            model_embeddings,
            index,
            chunks,
            k=3
        )
        context = " ".join(docs)
        answer = generate_response(item["question"])
        if answer in context:
            faithful += 1
    return faithful / len(eval_data)

print("Retrieval Accuracy:", retrieval_accuracy(evaluation_data))
print("Answer Accuracy:", answer_accuracy(evaluation_data))
print("Faithfulness Score:", faithfulness_score(evaluation_data))

from fastapi import FastAPI, Request, HTTPException
from pydantic import BaseModel

app = FastAPI(title="Arabic-English RAG API")

class Query(BaseModel):
    question: str

@app.post("/chat")
def chat(q: Query):
    return {
        "answer": generate_answer(q.question)
    }